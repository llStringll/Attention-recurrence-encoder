# Attention-recurrence-encoder
*Predicting next chars/words without using RecurrentNeuralNets, instead using MaskedAttention towards previous chars/words,thence predicting next chars/words, based on previous chars/words.
*This one is character-based.
*To run it, just use a text corpus, name it seqText.txt(or change the name inside the python script).
